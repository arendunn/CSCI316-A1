{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec95e8dd",
   "metadata": {},
   "source": [
    "# Task 1 - Classification with Custom Decision Trees\n",
    "\n",
    "This task utilises the Nursery Data Set and implements a decision tree classifier from scratch to preduct nursery application ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17823f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import csv\n",
    "from collections import Counter\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "54b76aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and read dataset\n",
    "def nursery_data(filepath='datasets/nursery.data'):\n",
    "    X, y = [], []\n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                X.append(row[:-1])\n",
    "                y.append(row[-1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92e61d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical data to integers\n",
    "def encode_data(X, y):\n",
    "    encoders = [{} for _ in range(len(X[0]))]\n",
    "    for col in range(len(X[0])):\n",
    "        unique_vals = sorted(set(row[col] for row in X))\n",
    "        encoders[col] = {val: i for i, val in enumerate(unique_vals)}\n",
    "        for row in X:\n",
    "            row[col] = encoders[col][row[col]]\n",
    "    label_map = {val: i for i, val in enumerate(sorted(set(y)))}\n",
    "    y = [label_map[label] for label in y]\n",
    "    return X, y, encoders, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57d483e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "def train_test_split(X, y, train_ratio=0.65):\n",
    "    combined = list(zip(X, y))\n",
    "    random.shuffle(combined)\n",
    "    X[:], y[:] = zip(*combined)\n",
    "    split_idx = int(len(X) * train_ratio)\n",
    "    return X[:split_idx], y[:split_idx], X[split_idx:], y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab41088",
   "metadata": {},
   "source": [
    "2. Create the Decision Tree Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1d013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decision Tree Classifier\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, children=None, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.children = children or {}\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.prediction is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1f52887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy & Info Gain\n",
    "def entropy(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    return -sum((c/total) * math.log2(c/total) for c in counts.values())\n",
    "\n",
    "def info_gain_tree(parent, branches):\n",
    "    total = len(parent)\n",
    "    weighted = sum(len(b)/total * entropy(b) for b in branches)\n",
    "    return entropy(parent) - weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "76c74a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Index\n",
    "def gini_index(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    return 1 - sum((c/total)**2 for c in counts.values())\n",
    "\n",
    "def gini_gain(parent, branches):\n",
    "    total = len(parent)\n",
    "    return -sum(len(b)/total * gini_index(b) for b in branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c029f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Split\n",
    "def split_by_feature(X, y, feature):\n",
    "    result = {}\n",
    "    for xi, label in zip(X, y):\n",
    "        key = xi[feature]\n",
    "        if key not in result:\n",
    "            result[key] = ([], [])\n",
    "        result[key][0].append(xi)\n",
    "        result[key][1].append(label)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b9a22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Builder\n",
    "def build_tree(X, y, criterion='info_gain'):\n",
    "    if len(set(y)) == 1:\n",
    "        return TreeNode(prediction=y[0])\n",
    "\n",
    "    if not X[0]:\n",
    "        return TreeNode(prediction=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "    best_gain = -float('inf')\n",
    "    best_feature = None\n",
    "    best_splits = None\n",
    "\n",
    "    for i in range(len(X[0])):\n",
    "        splits = split_by_feature(X, y, i)\n",
    "        branches = [labels for _, labels in splits.values()]\n",
    "        gain = info_gain_tree(y, branches) if criterion == 'info_gain' else gini_gain(y, branches)\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = i\n",
    "            best_splits = splits\n",
    "\n",
    "    if best_feature is None or best_gain <= 0:\n",
    "        return TreeNode(prediction=Counter(y).most_common(1)[0][0])\n",
    "\n",
    "    children = {}\n",
    "    for val, (x_subset, y_subset) in best_splits.items():\n",
    "        children[val] = build_tree(x_subset, y_subset, criterion)\n",
    "\n",
    "    return TreeNode(feature=best_feature, children=children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "30fe0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the classifier\n",
    "def predict(tree, sample):\n",
    "    while not tree.is_leaf():\n",
    "        val = sample[tree.feature]\n",
    "        if val in tree.children:\n",
    "            tree = tree.children[val]\n",
    "        else:\n",
    "            return None\n",
    "    return tree.prediction\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp) / len(y_true)\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, labels):\n",
    "    matrix = [[0]*len(labels) for _ in labels]\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yp is not None:\n",
    "            matrix[yt][yp] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "955cdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Voting\n",
    "def ensemble_predict(sample, trees, weights):\n",
    "    votes = {}\n",
    "    for tree, weight in zip(trees, weights):\n",
    "        pred = predict(tree, sample)\n",
    "        if pred is not None:\n",
    "            votes[pred] = votes.get(pred, 0) + weight\n",
    "    return max(votes.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e7f8dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree Printing\n",
    "def print_tree(node, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "    if node.is_leaf():\n",
    "        print(f\"{indent}Predict: {node.prediction}\")\n",
    "    else:\n",
    "        for val, child in node.children.items():\n",
    "            print(f\"{indent}If feature[{node.feature}] == {val}:\")\n",
    "            print_tree(child, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2320cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain Tree Accuracy: 0.9717813051146384\n",
      "Gini Index Accuracy: 0.32429453262786595\n",
      "Ensemble Accuracy: 0.9832451499118166\n",
      "\n",
      "Confusion Matrix (Information Gain):\n",
      "[1526, 0, 0, 0, 0]\n",
      "[0, 1402, 0, 11, 6]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 7, 0, 1398, 0]\n",
      "[0, 5, 3, 0, 82]\n",
      "\n",
      "Confusion Matrix (Gini Index):\n",
      "[0, 1526, 0, 0, 0]\n",
      "[0, 1471, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 1438, 0, 0, 0]\n",
      "[0, 100, 0, 0, 0]\n",
      "\n",
      "Confusion Matrix (Ensemble):\n",
      "[1526, 0, 0, 0, 0]\n",
      "[0, 1454, 0, 11, 6]\n",
      "[0, 0, 0, 0, 1]\n",
      "[0, 40, 0, 1398, 0]\n",
      "[0, 15, 3, 0, 82]\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "X, y = nursery_data()\n",
    "X, y, encoders, label_map = encode_data(X, y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "\n",
    "# Train both trees\n",
    "tree_info = build_tree(X_train, y_train, criterion='info_gain')\n",
    "tree_gini = build_tree(X_train, y_train, criterion='gini')\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_info = [predict(tree_info, x) for x in X_test]\n",
    "y_pred_gini = [predict(tree_gini, x) for x in X_test]\n",
    "\n",
    "acc_info = accuracy(y_test, y_pred_info)\n",
    "acc_gini = accuracy(y_test, y_pred_gini)\n",
    "\n",
    "print(\"Information Gain Tree Accuracy:\", acc_info)\n",
    "print(\"Gini Index Accuracy:\", acc_gini)\n",
    "\n",
    "# Ensemble voting\n",
    "ensemble_preds = [ensemble_predict(x, [tree_info, tree_gini], weights=[0.5, 0.5]) for x in X_test]\n",
    "ensemble_acc = accuracy(y_test, ensemble_preds)\n",
    "print(\"Ensemble Accuracy:\", ensemble_acc)\n",
    "\n",
    "# Confusion Matrix\n",
    "labels = sorted(set(y))\n",
    "matrix_info = confusion_matrix(y_test, y_pred_info, labels)\n",
    "matrix_gini = confusion_matrix(y_test, y_pred_gini, labels)\n",
    "matrix_ensemble = confusion_matrix(y_test, ensemble_preds, labels)\n",
    "\n",
    "# Simple print\n",
    "print(\"\\nConfusion Matrix (Information Gain):\")\n",
    "for row in matrix_info:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nConfusion Matrix (Gini Index):\")\n",
    "for row in matrix_gini:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nConfusion Matrix (Ensemble):\")\n",
    "for row in matrix_ensemble:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cc698",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Information Gain Tree (around 97%) and Ensemble (around 98%) were much more accurate than the Gini Index Tree (32%)\n",
    "- Information Gain showed high accuracy for most classes except priority and spec_prior\n",
    "- Gini Index Tree showed well enough accuracy for classes other than the spec_prior\n",
    "- Ensemble Tree improved in priority and spec_prior classes but showed some struggle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
